{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CENFARG/Excel_IA_Interprete/blob/main/excelIAPandas_OPENAI_v02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "vymNifbbhCuk",
        "outputId": "9412d595-7bce-4a83-e715-4adc5b0edaa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.23.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.11-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-experimental\n",
            "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.47)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.21 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.21)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.18)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.29.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting langchain-core<1.0.0,>=0.3.45 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.49-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.68.2)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (0.3.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.23.1-py3-none-any.whl (51.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.11-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.49-py3-none-any.whl (420 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m420.1/420.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, python-dotenv, mypy-extensions, marshmallow, httpx-sse, groovy, ffmpy, aiofiles, typing-inspect, tiktoken, starlette, safehttpx, pydantic-settings, gradio-client, fastapi, dataclasses-json, langchain-core, gradio, langchain-openai, langchain-community, langchain-experimental\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.47\n",
            "    Uninstalling langchain-core-0.3.47:\n",
            "      Successfully uninstalled langchain-core-0.3.47\n",
            "Successfully installed aiofiles-23.2.1 dataclasses-json-0.6.7 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.23.1 gradio-client-1.8.0 groovy-0.1.2 httpx-sse-0.4.0 langchain-community-0.3.20 langchain-core-0.3.49 langchain-experimental-0.3.4 langchain-openai-0.3.11 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 pydub-0.25.1 python-dotenv-1.1.0 python-multipart-0.0.20 ruff-0.11.2 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tiktoken-0.9.0 tomlkit-0.13.2 typing-inspect-0.9.0 uvicorn-0.34.0\n",
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://8eb87309ddfd9cfbfc.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8eb87309ddfd9cfbfc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"excelIAPandas_OPENAI_v02.ipynb\n",
        "\n",
        "Dev: Gonzalo F. Recalde\n",
        "Empresa: CENF www.cenfarg.com\n",
        "email: capacitaciones.cenf@gmail.com\n",
        "Coautor: Pablo A. Bonnecaze\n",
        "\n",
        "La API de OPen IA tiene 1Millon de tokes de uso, luego se bloqueara.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "!pip install langchain-community gradio pandas matplotlib openpyxl tabulate langchain-openai langchain-experimental\n",
        "\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import traceback\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import base64\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
        "from langchain.agents.agent_types import AgentType\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.output_parsers import PandasDataFrameOutputParser\n",
        "\n",
        "# Variables globales\n",
        "history = []  # Lista de mensajes en formato {role, content}\n",
        "last_df = None  # DataFrame global para uso en varias funciones\n",
        "\n",
        "def setup_agent(api_key, file):\n",
        "    print(\"[INFO] Configurando agente...\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "    if file is None:\n",
        "        raise ValueError(\"No se ha proporcionado un archivo válido.\")\n",
        "\n",
        "    df = pd.read_excel(file.name) if hasattr(file, 'name') else pd.read_excel(file)\n",
        "    print(f\"[INFO] Archivo cargado: {file.name if hasattr(file, 'name') else 'Archivo subido'}, Filas: {df.shape[0]}, Columnas: {df.shape[1]}\")\n",
        "\n",
        "    model = \"gpt-4o-mini\"\n",
        "    llm_model = ChatOpenAI(model=model, temperature=0, verbose=True, streaming=True)\n",
        "\n",
        "    # Crear un agente mejorado con más opciones\n",
        "    agent = create_pandas_dataframe_agent(\n",
        "        llm=llm_model,\n",
        "        df=df,\n",
        "        verbose=True,\n",
        "        agent_type=AgentType.OPENAI_FUNCTIONS,\n",
        "        return_intermediate_steps=True,\n",
        "        allow_dangerous_code=True,\n",
        "        # handle_parsing_errors=True\n",
        "    )\n",
        "    return agent, df, llm_model\n",
        "\n",
        "def clean_code(code):\n",
        "    # Si el código contiene bloques de código markdown\n",
        "    if \"```\" in code:\n",
        "        parts = code.split(\"```\")\n",
        "        for part in parts:\n",
        "            if \"df[\" in part or \"df.\" in part or \"pivot_table\" in part:\n",
        "                code = part.strip()\n",
        "                # Eliminar etiqueta de lenguaje\n",
        "                if code.startswith(\"python\"):\n",
        "                    code = code[6:].strip()\n",
        "                break\n",
        "\n",
        "    # Eliminar comentarios y líneas vacías\n",
        "    cleaned_lines = []\n",
        "    for line in code.split(\"\\n\"):\n",
        "        if line.strip() and not line.strip().startswith(\"#\"):\n",
        "            cleaned_lines.append(line)\n",
        "\n",
        "    return \"\\n\".join(cleaned_lines)\n",
        "\n",
        "def query_agent(api_key, file, query, send_full_history):\n",
        "    global history, last_df\n",
        "    print(f\"[INFO] Prompt del usuario para consulta común: {query}\")\n",
        "    excel_path = None\n",
        "\n",
        "    if not api_key or file is None:\n",
        "        print(\"[ERROR] Falta API Key o archivo\")\n",
        "        return [], None, \"\"\n",
        "\n",
        "    try:\n",
        "        print(f\"[INFO] Recibida consulta: {query}\")\n",
        "        agent, df, llm_model = setup_agent(api_key, file)\n",
        "        last_df = df\n",
        "\n",
        "        # Preparar contexto con historial si es necesario\n",
        "        if send_full_history and history:\n",
        "            context = \"Historial de conversación:\\n\"\n",
        "            for i in range(0, len(history), 2):\n",
        "                if i+1 < len(history):\n",
        "                    context += f\"Usuario: {history[i]['content']}\\n\"\n",
        "                    context += f\"Asistente: {history[i+1]['content']}\\n\\n\"\n",
        "            context += \"Nueva consulta: \" + query\n",
        "            print(f\"[INFO] Enviando consulta con contexto completo\")\n",
        "\n",
        "            # Crear un prompt template para incluir el contexto\n",
        "            prompt_template = PromptTemplate(\n",
        "                template=\"Eres un asistente experto en análisis de datos. Analiza el siguiente DataFrame y responde a la consulta del usuario.\\n\\nContexto previo:\\n{context}\\n\\nConsulta: {query}\\n\\nAsegúrate de proporcionar respuestas claras y concisas, con datos numéricos cuando sea relevante.\",\n",
        "                input_variables=[\"context\", \"query\"]\n",
        "            )\n",
        "\n",
        "            # Usar el prompt template\n",
        "            formatted_prompt = prompt_template.format(context=context, query=query)\n",
        "            response = agent.invoke(formatted_prompt)\n",
        "        else:\n",
        "            # Crear un prompt template para consultas individuales\n",
        "            prompt_template = PromptTemplate(\n",
        "                template=\"Eres un asistente experto en análisis de datos. Analiza el siguiente DataFrame y responde a la consulta del usuario.\\n\\nConsulta: {query}\\n\\nAsegúrate de proporcionar respuestas claras y concisas, con datos numéricos cuando sea relevante.\",\n",
        "                input_variables=[\"query\"]\n",
        "            )\n",
        "\n",
        "            # Usar el prompt template\n",
        "            formatted_prompt = prompt_template.format(query=query)\n",
        "            response = agent.invoke(formatted_prompt)\n",
        "\n",
        "        if isinstance(response, dict) and \"output\" in response:\n",
        "            response = response[\"output\"]\n",
        "\n",
        "        # Generar Excel para consultas que impliquen mostrar datos\n",
        "        if \"mostrar\" in query.lower() or \"listar\" in query.lower() or \"obtener\" in query.lower():\n",
        "            try:\n",
        "                if \"primeras\" in query.lower() or \"primeros\" in query.lower():\n",
        "                    n = int(''.join(filter(str.isdigit, query.split(\"primeras\")[1].split()[0])))\n",
        "                    result_df = df.head(n)\n",
        "                elif \"últimas\" in query.lower() or \"ultimas\" in query.lower():\n",
        "                    n = int(''.join(filter(str.isdigit, query.split(\"últimas\")[1].split()[0])))\n",
        "                    result_df = df.tail(n)\n",
        "                else:\n",
        "                    result_df = df.head(10)\n",
        "\n",
        "                excel_path = \"resultados.xlsx\"\n",
        "                result_df.to_excel(excel_path, index=False)\n",
        "                print(f\"[INFO] Archivo Excel generado: {excel_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR] No se pudo generar Excel: {str(e)}\")\n",
        "\n",
        "        # Si la respuesta es un DataFrame\n",
        "        if isinstance(response, pd.DataFrame):\n",
        "            excel_path = \"resultados.xlsx\"\n",
        "            response.to_excel(excel_path, index=False)\n",
        "            response_text = response.to_string(index=False)\n",
        "\n",
        "            # Actualizar historial con formato correcto\n",
        "            new_messages = [\n",
        "                {\"role\": \"user\", \"content\": query},\n",
        "                {\"role\": \"assistant\", \"content\": response_text}\n",
        "            ]\n",
        "            history.extend(new_messages)\n",
        "            # Devolver todo el historial en lugar de solo los nuevos mensajes\n",
        "            return history, excel_path, \"\"\n",
        "\n",
        "        # Respuesta normal\n",
        "        new_messages = [\n",
        "            {\"role\": \"user\", \"content\": query},\n",
        "            {\"role\": \"assistant\", \"content\": response}\n",
        "        ]\n",
        "        history.extend(new_messages)\n",
        "        print(f\"[INFO] Respuesta generada: {response}\")\n",
        "        return new_messages, excel_path, \"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] Se produjo un error en la consulta\")\n",
        "        traceback.print_exc()\n",
        "        new_messages = [\n",
        "            {\"role\": \"user\", \"content\": query},\n",
        "            {\"role\": \"assistant\", \"content\": f\"Error: {str(e)}\"}\n",
        "        ]\n",
        "        history.extend(new_messages)\n",
        "        return new_messages, None, \"\"\n",
        "\n",
        "def save_conversation():\n",
        "    global history\n",
        "    if not history:\n",
        "        return None\n",
        "\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    file_path = f\"historial_conversacion_{timestamp}.txt\"\n",
        "\n",
        "    conversation = \"\"\n",
        "    for i in range(0, len(history), 2):\n",
        "        if i+1 < len(history):\n",
        "            conversation += f\"Usuario: {history[i]['content']}\\n\"\n",
        "            conversation += f\"Asistente: {history[i+1]['content']}\\n---\\n\"\n",
        "\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(conversation)\n",
        "    print(f\"[INFO] Conversación guardada en {file_path}\")\n",
        "    return file_path\n",
        "\n",
        "def generate_default_excel():\n",
        "    global last_df\n",
        "    if last_df is None:\n",
        "        return None\n",
        "\n",
        "    excel_path = \"resultados_default.xlsx\"\n",
        "    last_df.head(5).to_excel(excel_path, index=False)\n",
        "    print(f\"[INFO] Archivo Excel generado por defecto: {excel_path}\")\n",
        "    return excel_path\n",
        "\n",
        "def generate_data_profile(file, api_key, additional_info):\n",
        "    global history\n",
        "    print(f\"[INFO] Prompt del usuario para perfil de datos: Generar perfil\")\n",
        "    print(f\"[INFO] Información adicional: {additional_info}\")\n",
        "    if file is None:\n",
        "        return [], None\n",
        "\n",
        "    try:\n",
        "        # Cargar el DataFrame\n",
        "        _, df, llm_model = setup_agent(api_key, file)\n",
        "\n",
        "        # Crear un prompt template para incluir la información adicional\n",
        "        # Obtener información detallada sobre las columnas\n",
        "        column_info = {col: {\n",
        "            'dtype': str(df[col].dtype),\n",
        "            'n_unique': df[col].nunique(),\n",
        "            'n_missing': df[col].isnull().sum(),\n",
        "            'is_numeric': pd.api.types.is_numeric_dtype(df[col])\n",
        "        } for col in df.columns}\n",
        "\n",
        "        prompt_template = PromptTemplate(\n",
        "            template=\"\"\"Eres un experto en análisis de datos y visualización.\n",
        "\n",
        "            Tengo un DataFrame con la siguiente información de columnas:\n",
        "            {column_details}\n",
        "\n",
        "            El usuario ha solicitado lo siguiente:\n",
        "            \"{additional_info}\"\n",
        "\n",
        "            Genera un análisis detallado que tenga en cuenta las necesidades específicas del usuario.\n",
        "            Incluye recomendaciones sobre:\n",
        "            1. Qué columnas son más relevantes según la solicitud del usuario\n",
        "            2. Qué análisis estadísticos específicos serían más útiles\n",
        "            3. Si se detectan problemas de calidad en los datos que afecten al análisis\n",
        "            4. Sugerencias de visualizaciones o análisis adicionales\n",
        "\n",
        "            Estructura tu respuesta en secciones claramente definidas.\"\"\",\n",
        "            input_variables=[\"column_details\", \"additional_info\"]\n",
        "        )\n",
        "\n",
        "        # Formatear el prompt con información detallada\n",
        "        formatted_prompt = prompt_template.format(\n",
        "            column_details=\"\\n\".join(\n",
        "                f\"- {col}: tipo={info['dtype']}, valores únicos={info['n_unique']}, \"\n",
        "                f\"valores nulos={info['n_missing']}, es_numérico={info['is_numeric']}\"\n",
        "                for col, info in column_info.items()\n",
        "            ),\n",
        "            additional_info=additional_info\n",
        "        )\n",
        "\n",
        "        # Enviar el prompt al modelo y obtener respuesta\n",
        "        response = llm_model.invoke(formatted_prompt)\n",
        "        analysis_text = response.content\n",
        "        print(f\"[INFO] Análisis del modelo: {analysis_text}\")\n",
        "\n",
        "        # Preparar análisis estadístico personalizado basado en la respuesta del modelo\n",
        "        stats = pd.DataFrame()\n",
        "\n",
        "        # Determinar columnas numéricas y categóricas\n",
        "        numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "        # Estadísticas básicas para columnas numéricas\n",
        "        if not numeric_cols.empty:\n",
        "            stats = pd.concat([stats, df[numeric_cols].describe()], axis=1)\n",
        "\n",
        "        # Estadísticas para columnas categóricas\n",
        "        if not categorical_cols.empty:\n",
        "            cat_stats = pd.DataFrame({\n",
        "                col: {\n",
        "                    'unique_count': df[col].nunique(),\n",
        "                    'top_value': df[col].mode()[0] if not df[col].empty else None,\n",
        "                    'top_freq': df[col].value_counts().iloc[0] if not df[col].empty else 0,\n",
        "                    'null_count': df[col].isnull().sum()\n",
        "                } for col in categorical_cols\n",
        "            })\n",
        "            stats = pd.concat([stats, cat_stats], axis=1)\n",
        "\n",
        "        # Información sobre valores nulos\n",
        "        null_counts = df.isnull().sum()\n",
        "        null_percentage = (null_counts / len(df)) * 100\n",
        "        null_info = pd.DataFrame({\n",
        "            'Valores Nulos': null_counts,\n",
        "            '% Nulos': null_percentage\n",
        "        })\n",
        "\n",
        "        # Tipos de datos\n",
        "        dtypes = pd.DataFrame(df.dtypes, columns=['Tipo de Dato'])\n",
        "\n",
        "        # Unir toda la información\n",
        "        profile_data = pd.concat([dtypes, null_info], axis=1)\n",
        "\n",
        "        # Generar Excel con el perfil mejorado\n",
        "        profile_path = \"perfil_datos.xlsx\"\n",
        "        with pd.ExcelWriter(profile_path) as writer:\n",
        "            # Hoja principal con el análisis del modelo\n",
        "            pd.DataFrame({\n",
        "                'Análisis del Modelo': analysis_text.split('\\n')\n",
        "            }).to_excel(writer, sheet_name='Análisis', index=False)\n",
        "\n",
        "            # Información de columnas con detalles\n",
        "            column_details = pd.DataFrame(column_info).T\n",
        "            column_details.index.name = 'Columna'\n",
        "            column_details.to_excel(writer, sheet_name='Información Columnas')\n",
        "\n",
        "            # Estadísticas\n",
        "            stats.to_excel(writer, sheet_name='Estadísticas')\n",
        "\n",
        "            # Correlaciones para columnas numéricas si existen\n",
        "            if len(numeric_cols) > 1:\n",
        "                df[numeric_cols].corr().to_excel(writer, sheet_name='Correlaciones')\n",
        "\n",
        "            # Muestra de datos\n",
        "            df.head(10).to_excel(writer, sheet_name='Muestra')\n",
        "\n",
        "        if analysis_text == \"\":\n",
        "            analysis_text = \"No se proporcionaron condiciones adicionales para el análisis personalizado.\"\n",
        "\n",
        "        # Generar texto descriptivo mejorado\n",
        "        # Análisis detallado de tipos de datos\n",
        "        dtypes_summary = df.dtypes.value_counts().to_dict()\n",
        "        dtype_info = \"\\n\".join([f\"- **{dtype}**: {count} columnas\" for dtype, count in dtypes_summary.items()])\n",
        "\n",
        "        # Análisis de valores únicos para columnas categóricas\n",
        "        categorical_info = []\n",
        "        if not categorical_cols.empty:\n",
        "            for col in categorical_cols[:5]:  # Limitamos a 5 columnas para no sobrecargar\n",
        "                unique_values = df[col].nunique()\n",
        "                total_values = len(df)\n",
        "                categorical_info.append(\n",
        "                    f\"- **{col}**: {unique_values} valores únicos \"\n",
        "                    f\"({(unique_values/total_values*100):.1f}% de cardinalidad)\"\n",
        "                )\n",
        "\n",
        "        # Análisis de rangos para columnas numéricas\n",
        "        numeric_info = []\n",
        "        if not numeric_cols.empty:\n",
        "            for col in numeric_cols[:5]:  # Limitamos a 5 columnas para no sobrecargar\n",
        "                stats = df[col].describe()\n",
        "                numeric_info.append(\n",
        "                    f\"- **{col}**: rango [{stats['min']:.2f} - {stats['max']:.2f}], \"\n",
        "                    f\"media: {stats['mean']:.2f}, mediana: {stats['50%']:.2f}\"\n",
        "                )\n",
        "\n",
        "        # Identificar columnas con más valores nulos\n",
        "        null_cols = df.isnull().sum()\n",
        "        problematic_cols = null_cols[null_cols > 0].sort_values(ascending=False)\n",
        "        null_info = []\n",
        "        if not problematic_cols.empty:\n",
        "            for col, nulls in problematic_cols.items():\n",
        "                percentage = (nulls / len(df)) * 100\n",
        "                if percentage > 5:  # Solo mostrar columnas con más del 5% de nulos\n",
        "                    null_info.append(f\"- **{col}**: {nulls} nulos ({percentage:.1f}%)\")\n",
        "\n",
        "        summary = [\n",
        "            f\"### Perfil de datos para {len(df):,} filas y {len(df.columns)} columnas\",\n",
        "            \"\",\n",
        "            \"#### Resumen General:\",\n",
        "            f\"- **Dimensiones**: {df.shape[0]:,} × {df.shape[1]} (filas × columnas)\",\n",
        "            f\"- **Tipos de datos**:\\n{dtype_info}\",\n",
        "            f\"- **Valores nulos totales**: {df.isnull().sum().sum():,} ({df.isnull().sum().sum() / (df.shape[0] * df.shape[1]):.2%})\",\n",
        "        ]\n",
        "\n",
        "        if null_info:\n",
        "            summary.extend([\n",
        "                \"\",\n",
        "                \"#### Columnas con Valores Nulos Significativos:\",\n",
        "                *null_info\n",
        "            ])\n",
        "\n",
        "        if numeric_info:\n",
        "            summary.extend([\n",
        "                \"\",\n",
        "                \"#### Resumen de Columnas Numéricas Principales:\",\n",
        "                *numeric_info\n",
        "            ])\n",
        "\n",
        "        if categorical_info:\n",
        "            summary.extend([\n",
        "                \"\",\n",
        "                \"#### Resumen de Columnas Categóricas Principales:\",\n",
        "                *categorical_info\n",
        "            ])\n",
        "\n",
        "        summary.extend([\n",
        "            \"\",\n",
        "            \"#### Análisis Personalizado:\",\n",
        "            analysis_text,\n",
        "            \"\",\n",
        "            \"Se ha generado un archivo Excel con el perfil completo de los datos y análisis detallados.\"\n",
        "        ])\n",
        "\n",
        "        message = \"\\n\".join(summary)\n",
        "        new_messages = [\n",
        "            {\"role\": \"user\", \"content\": \"Generar perfil de datos\"},\n",
        "            {\"role\": \"assistant\", \"content\": message}\n",
        "        ]\n",
        "        history.extend(new_messages)\n",
        "        return new_messages, profile_path\n",
        "\n",
        "    except Exception as e:\n",
        "        traceback.print_exc()\n",
        "        new_messages = [\n",
        "            {\"role\": \"user\", \"content\": \"Generar perfil de datos\"},\n",
        "            {\"role\": \"assistant\", \"content\": f\"Error: {str(e)}\"}\n",
        "        ]\n",
        "        history.extend(new_messages)\n",
        "        return new_messages, None\n",
        "\n",
        "def compare_columns(file, api_key, col1, col2, additional_conditions):\n",
        "    global history\n",
        "    print(f\"[INFO] Prompt del usuario para comparador de columnas: Comparar {col1} y {col2}\")\n",
        "    print(f\"[INFO] Condiciones adicionales: {additional_conditions}\")\n",
        "    if file is None:\n",
        "        return [], None\n",
        "\n",
        "    try:\n",
        "        # Cargar el DataFrame\n",
        "        _, df, _ = setup_agent(api_key, file)\n",
        "\n",
        "        if col1 not in df.columns or col2 not in df.columns:\n",
        "            error_msg = f\"Error: Una o ambas columnas ({col1}, {col2}) no están en el dataset\"\n",
        "            new_messages = [\n",
        "                {\"role\": \"user\", \"content\": f\"Comparar columnas '{col1}' y '{col2}'\"},\n",
        "                {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            ]\n",
        "            history.extend(new_messages)\n",
        "            return new_messages, None\n",
        "\n",
        "        # Verificar si ambas columnas son numéricas\n",
        "        numeric_comparison = pd.api.types.is_numeric_dtype(df[col1]) and pd.api.types.is_numeric_dtype(df[col2])\n",
        "\n",
        "        # Calcular correlación si son numéricas\n",
        "        if numeric_comparison:\n",
        "            correlation = df[col1].corr(df[col2])\n",
        "        else:\n",
        "            correlation = None\n",
        "\n",
        "        # Generar gráfico\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        if numeric_comparison:\n",
        "            plt.scatter(df[col1], df[col2], alpha=0.5)\n",
        "            plt.title(f\"Relación entre {col1} y {col2}\")\n",
        "            plt.xlabel(col1)\n",
        "            plt.ylabel(col2)\n",
        "            plt.grid(True, alpha=0.3)\n",
        "        else:\n",
        "            # Para columnas categóricas o mixtas\n",
        "            cross_tab = pd.crosstab(df[col1], df[col2])\n",
        "            cross_tab.plot(kind='bar', stacked=True)\n",
        "            plt.title(f\"Distribución de {col2} por {col1}\")\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.legend(title=col2)\n",
        "\n",
        "        # Convertir gráfico a imagen\n",
        "        buf = io.BytesIO()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(buf, format='png', dpi=100)\n",
        "        buf.seek(0)\n",
        "        img_str = base64.b64encode(buf.read()).decode('utf-8')\n",
        "        plt.close()\n",
        "\n",
        "        # Generar estadísticas\n",
        "        stats = []\n",
        "        stats.append(f\"### Comparación entre '{col1}' y '{col2}'\")\n",
        "\n",
        "        if numeric_comparison:\n",
        "            stats.append(f\"**Correlación**: {correlation:.4f}\")\n",
        "            stats.append(f\"**Valores únicos en '{col1}'**: {df[col1].nunique()}\")\n",
        "            stats.append(f\"**Valores únicos en '{col2}'**: {df[col2].nunique()}\")\n",
        "        else:\n",
        "            stats.append(f\"**Valores únicos en '{col1}'**: {df[col1].nunique()}\")\n",
        "            stats.append(f\"**Valores únicos en '{col2}'**: {df[col2].nunique()}\")\n",
        "            stats.append(\"*No se puede calcular correlación entre variables no numéricas*\")\n",
        "\n",
        "        # Generar Excel con datos de comparación\n",
        "        comparison_path = \"comparacion_columnas.xlsx\"\n",
        "        with pd.ExcelWriter(comparison_path) as writer:\n",
        "            if numeric_comparison:\n",
        "                df[[col1, col2]].describe().to_excel(writer, sheet_name='Estadísticas')\n",
        "\n",
        "            # Tabla de contingencia\n",
        "            pd.crosstab(df[col1], df[col2]).to_excel(writer, sheet_name='Tabla Cruzada')\n",
        "\n",
        "            # Muestra de datos\n",
        "            df[[col1, col2]].head(100).to_excel(writer, sheet_name='Muestra')\n",
        "\n",
        "        message = \"\\n\".join(stats) + f\"\\n\\n![Comparación](data:image/png;base64,{img_str})\"\n",
        "        new_messages = [\n",
        "            {\"role\": \"user\", \"content\": f\"Comparar columnas '{col1}' y '{col2}'\"},\n",
        "            {\"role\": \"assistant\", \"content\": message}\n",
        "        ]\n",
        "        history.extend(new_messages)\n",
        "        return history, comparison_path\n",
        "\n",
        "    except Exception as e:\n",
        "        traceback.print_exc()\n",
        "        new_messages = [\n",
        "            {\"role\": \"user\", \"content\": f\"Comparar columnas '{col1}' y '{col2}'\"},\n",
        "            {\"role\": \"assistant\", \"content\": f\"Error: {str(e)}\"}\n",
        "        ]\n",
        "        history.extend(new_messages)\n",
        "        # Devolver todo el historial en lugar de solo los nuevos mensajes\n",
        "        return history, None\n",
        "\n",
        "def advanced_search(file, api_key, query):\n",
        "    global history\n",
        "    print(f\"[INFO] Prompt del usuario para búsqueda avanzada: {query}\")\n",
        "    if file is None:\n",
        "        return [], None\n",
        "\n",
        "    try:\n",
        "        # Cargar el DataFrame\n",
        "        _, df, llm_model = setup_agent(api_key, file)\n",
        "\n",
        "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "        # Usar PromptTemplate para mejorar la generación de código\n",
        "        prompt_template = PromptTemplate(\n",
        "            template=\"\"\"Eres un experto en pandas y análisis de datos.\n",
        "\n",
        "            Estoy trabajando con un DataFrame de pandas con estas columnas: {columns}\n",
        "\n",
        "            Quiero buscar datos que cumplan con lo siguiente: \"{query}\"\n",
        "\n",
        "            Genera solo el código Python que usaría para filtrar el DataFrame (df) basado en esta consulta.\n",
        "            El resultado debe ser una condición booleana que se pueda usar en df[...] para filtrar el DataFrame.\n",
        "            IMPORTANTE: NO uses triple backticks ni etiquetas de lenguaje. Devuelve SOLO el código Python puro.\"\"\",\n",
        "            input_variables=[\"columns\", \"query\"]\n",
        "        )\n",
        "\n",
        "        formatted_prompt = prompt_template.format(\n",
        "            columns=\", \".join(df.columns.tolist()),\n",
        "            query=query\n",
        "        )\n",
        "\n",
        "        response = llm_model.invoke(formatted_prompt)\n",
        "        raw_code = response.content.strip()\n",
        "        filter_code = clean_code(raw_code)\n",
        "\n",
        "        print(f\"[INFO] Código filtro limpio: {filter_code}\")\n",
        "\n",
        "        # Si la consulta es solo para mostrar filas, usamos head()\n",
        "        if \"primeras\" in query.lower() and \"filas\" in query.lower():\n",
        "            try:\n",
        "                n = int(''.join(filter(str.isdigit, query)))\n",
        "                n = n if n > 0 else 10\n",
        "                filtered_df = df.head(n)\n",
        "            except:\n",
        "                filtered_df = df.head(10)  # valor por defecto\n",
        "        else:\n",
        "            # Intentar ejecutar el código directamente\n",
        "            try:\n",
        "                # Si el código comienza con df[\n",
        "                if filter_code.startswith(\"df[\"):\n",
        "                    filtered_df = eval(filter_code)\n",
        "                # Si es solo una condición\n",
        "                elif \">\" in filter_code or \"<\" in filter_code or \"==\" in filter_code or \".str\" in filter_code:\n",
        "                    filtered_df = df[eval(filter_code)]\n",
        "                else:\n",
        "                    # Último recurso: ejecutar como está\n",
        "                    filtered_df = eval(f\"df[{filter_code}]\")\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR] Error al filtrar: {str(e)}\")\n",
        "                # Intentar una búsqueda simple por defecto\n",
        "                if any(col in query.lower() for col in [c.lower() for c in df.columns]):\n",
        "                    for col in df.columns:\n",
        "                        if col.lower() in query.lower():\n",
        "                            search_term = query.lower().split(col.lower())[1].strip()\n",
        "                            if search_term:\n",
        "                                filtered_df = df[df[col].astype(str).str.contains(search_term, case=False)]\n",
        "                                break\n",
        "                else:\n",
        "                    # Si todo falla, mostrar las primeras 10 filas\n",
        "                    filtered_df = df.head(10)\n",
        "\n",
        "        # Generar Excel con resultados\n",
        "        result_path = \"resultados_busqueda.xlsx\"\n",
        "        filtered_df.to_excel(result_path, index=False)\n",
        "\n",
        "        message = f\"Se encontraron {len(filtered_df)} registros que coinciden con la búsqueda.\\n\\nConsulta: '{query}'\"\n",
        "        new_messages = [\n",
        "            {\"role\": \"user\", \"content\": f\"Buscar '{query}'\"},\n",
        "            {\"role\": \"assistant\", \"content\": message}\n",
        "        ]\n",
        "        history.extend(new_messages)\n",
        "        # Devolver todo el historial en lugar de solo los nuevos mensajes\n",
        "        return history, result_path\n",
        "\n",
        "    except Exception as e:\n",
        "        traceback.print_exc()\n",
        "        new_messages = [\n",
        "            {\"role\": \"user\", \"content\": f\"Buscar '{query}'\"},\n",
        "            {\"role\": \"assistant\", \"content\": f\"Error: {str(e)}\"}\n",
        "        ]\n",
        "        history.extend(new_messages)\n",
        "        return new_messages, None\n",
        "\n",
        "def natural_filter(file, api_key, filter_query):\n",
        "    global history\n",
        "    print(f\"[INFO] Prompt del usuario para filtro natural: {filter_query}\")\n",
        "    if file is None:\n",
        "        return [], None\n",
        "\n",
        "    try:\n",
        "        # Cargar el DataFrame\n",
        "        _, df, llm_model = setup_agent(api_key, file)\n",
        "\n",
        "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "        # Usar PromptTemplate para mejorar la generación de código\n",
        "        prompt_template = PromptTemplate(\n",
        "            template=\"\"\"Eres un experto en pandas y análisis de datos.\n",
        "\n",
        "            Tengo un DataFrame de pandas con estas columnas:\n",
        "            {column_info}\n",
        "\n",
        "            Aquí hay una muestra de los primeros registros:\n",
        "            {data_sample}\n",
        "\n",
        "            Quiero filtrar los datos según este criterio expresado en lenguaje natural:\n",
        "            \"{filter_query}\"\n",
        "\n",
        "            Genera el código Python que filtraría el DataFrame (df).\n",
        "            Asegúrate de que el código sea válido y completo.\n",
        "            IMPORTANTE: NO uses triple backticks ni etiquetas de lenguaje. Devuelve SOLO el código Python puro.\"\"\",\n",
        "            input_variables=[\"column_info\", \"data_sample\", \"filter_query\"]\n",
        "        )\n",
        "\n",
        "        formatted_prompt = prompt_template.format(\n",
        "            column_info=\"\\n\".join([f\"- {col} ({df[col].dtype})\" for col in df.columns]),\n",
        "            data_sample=df.head(3).to_string(),\n",
        "            filter_query=filter_query\n",
        "        )\n",
        "\n",
        "        response = llm_model.invoke(formatted_prompt)\n",
        "        raw_code = response.content.strip()\n",
        "        filter_code = clean_code(raw_code)\n",
        "\n",
        "        print(f\"[INFO] Código filtro limpio: {filter_code}\")\n",
        "\n",
        "        # Manejar casos específicos comunes\n",
        "        if \"salidas\" in filter_query.lower() and \"mayor\" in filter_query.lower():\n",
        "            # Extraer el valor numérico\n",
        "            import re\n",
        "            numbers = re.findall(r'\\d+', filter_query)\n",
        "            if numbers:\n",
        "                value = int(numbers[0])\n",
        "                if \"m\" in filter_query.lower() or \"millones\" in filter_query.lower():\n",
        "                    value *= 1_000_000\n",
        "                result_df = df[df['SALIDAS'] > value]\n",
        "            else:\n",
        "                result_df = df[df['SALIDAS'] > df['SALIDAS'].median()]\n",
        "        elif \"ingresos\" in filter_query.lower() and \"mayor\" in filter_query.lower():\n",
        "            # Extraer el valor numérico\n",
        "            import re\n",
        "            numbers = re.findall(r'\\d+', filter_query)\n",
        "            if numbers:\n",
        "                value = int(numbers[0])\n",
        "                if \"m\" in filter_query.lower() or \"millones\" in filter_query.lower():\n",
        "                    value *= 1_000_000\n",
        "                result_df = df[df['INGRESOS'] > value]\n",
        "            else:\n",
        "                result_df = df[df['INGRESOS'] > df['INGRESOS'].median()]\n",
        "        else:\n",
        "            # Intentar ejecutar el código directamente\n",
        "            try:\n",
        "                # Si el código comienza con df[\n",
        "                if filter_code.startswith(\"df[\"):\n",
        "                    result_df = eval(filter_code)\n",
        "                # Si es solo una condición\n",
        "                elif \">\" in filter_code or \"<\" in filter_code or \"==\" in filter_code or \".str\" in filter_code:\n",
        "                    result_df = df[eval(filter_code)]\n",
        "                else:\n",
        "                    # Último recurso: ejecutar como está\n",
        "                    result_df = eval(f\"df[{filter_code}]\")\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR] Error al filtrar: {str(e)}\")\n",
        "                # Intentar una búsqueda simple por defecto\n",
        "                if any(col.lower() in filter_query.lower() for col in df.columns):\n",
        "                    for col in df.columns:\n",
        "                        if col.lower() in filter_query.lower():\n",
        "                            if \"mayor\" in filter_query.lower():\n",
        "                                result_df = df[df[col] > df[col].median()]\n",
        "                            elif \"menor\" in filter_query.lower():\n",
        "                                result_df = df[df[col] < df[col].median()]\n",
        "                            else:\n",
        "                                result_df = df[df[col].notna()]\n",
        "                            break\n",
        "                else:\n",
        "                    # Si todo falla, mostrar las primeras 10 filas\n",
        "                    result_df = df.head(10)\n",
        "\n",
        "        # Generar Excel con resultados\n",
        "        result_path = \"datos_filtrados.xlsx\"\n",
        "        result_df.to_excel(result_path, index=False)\n",
        "\n",
        "        message = f\"Filtro aplicado: '{filter_query}'\\n\\nResultado: {len(result_df)} filas cumplen la condición.\"\n",
        "        new_messages = [\n",
        "            {\"role\": \"user\", \"content\": f\"Filtrar '{filter_query}'\"},\n",
        "            {\"role\": \"assistant\", \"content\": message}\n",
        "        ]\n",
        "        history.extend(new_messages)\n",
        "        # Devolver todo el historial en lugar de solo los nuevos mensajes\n",
        "        return history, result_path\n",
        "\n",
        "    except Exception as e:\n",
        "        traceback.print_exc()\n",
        "        new_messages = [\n",
        "            {\"role\": \"user\", \"content\": f\"Filtrar '{filter_query}'\"},\n",
        "            {\"role\": \"assistant\", \"content\": f\"Error: {str(e)}\"}\n",
        "        ]\n",
        "        history.extend(new_messages)\n",
        "        # Devolver todo el historial en lugar de solo los nuevos mensajes\n",
        "        return history, None\n",
        "\n",
        "def generate_pivot_table(file, api_key, pivot_query):\n",
        "    global history\n",
        "    print(f\"[INFO] Prompt del usuario para tabla dinámica: {pivot_query}\")\n",
        "    if file is None:\n",
        "        return [], None\n",
        "\n",
        "    try:\n",
        "        # Cargar el DataFrame y configurar el modelo\n",
        "        _, df, llm_model = setup_agent(api_key, file)\n",
        "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "        # Identificar columnas numéricas y categóricas\n",
        "        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "        tabla_dinamica = None  # Variable para almacenar el resultado final\n",
        "\n",
        "        try:\n",
        "            # Generar y limpiar el código para la tabla dinámica\n",
        "            prompt_template = PromptTemplate(\n",
        "                template=\"\"\"Eres un experto en pandas y análisis de datos.\n",
        "                Tengo un DataFrame de pandas con estas columnas:\n",
        "                Numéricas: {numeric_cols}\n",
        "                Categóricas: {categorical_cols}\n",
        "\n",
        "                Quiero crear una tabla dinámica basada en esta descripción:\n",
        "                \"{pivot_query}\"\n",
        "\n",
        "                Genera el código Python usando df.pivot_table().\n",
        "                IMPORTANTE: NO uses triple backticks. Devuelve SOLO el código Python puro.\"\"\",\n",
        "                input_variables=[\"numeric_cols\", \"categorical_cols\", \"pivot_query\"]\n",
        "            )\n",
        "\n",
        "            formatted_prompt = prompt_template.format(\n",
        "                numeric_cols=\", \".join(numeric_cols) if numeric_cols else \"No hay columnas numéricas\",\n",
        "                categorical_cols=\", \".join(categorical_cols) if categorical_cols else \"No hay columnas categóricas\",\n",
        "                pivot_query=pivot_query\n",
        "            )\n",
        "\n",
        "            response = llm_model.invoke(formatted_prompt)\n",
        "            pivot_code = clean_code(response.content.strip())\n",
        "            print(f\"[INFO] Código tabla dinámica limpio: {pivot_code}\")\n",
        "\n",
        "            # Ejecutar el código generado\n",
        "            local_vars = {'df': df, 'pd': pd}\n",
        "            if \"=\" in pivot_code:\n",
        "                exec(pivot_code, globals(), local_vars)\n",
        "                result_var = pivot_code.split('=')[0].strip()\n",
        "                tabla_dinamica = local_vars[result_var]\n",
        "            else:\n",
        "                tabla_dinamica = eval(pivot_code, globals(), {'df': df, 'pd': pd})\n",
        "\n",
        "        except Exception as code_error:\n",
        "            print(f\"[ERROR] Error al ejecutar código generado: {str(code_error)}\")\n",
        "\n",
        "            # Crear tabla dinámica automática si falla el código generado\n",
        "            print(\"[INFO] Intentando crear tabla dinámica automática...\")\n",
        "\n",
        "            values = []\n",
        "            if 'ingresos' in pivot_query.lower():\n",
        "                values.append('INGRESOS')\n",
        "            if 'salidas' in pivot_query.lower():\n",
        "                values.append('SALIDAS')\n",
        "            if not values and numeric_cols:\n",
        "                values = [numeric_cols[0]]\n",
        "\n",
        "            index = None\n",
        "            for col in categorical_cols:\n",
        "                if col.upper() in pivot_query.upper():\n",
        "                    index = col\n",
        "                    break\n",
        "            if not index and categorical_cols:\n",
        "                index = categorical_cols[0]\n",
        "\n",
        "            if not values or not index:\n",
        "                raise ValueError(\"No se pudieron determinar las columnas para la tabla dinámica\")\n",
        "\n",
        "            agg_func = 'sum' if 'sum' in pivot_query.lower() or 'total' in pivot_query.lower() else 'mean'\n",
        "\n",
        "            tabla_dinamica = df.pivot_table(\n",
        "                values=values,\n",
        "                index=index,\n",
        "                aggfunc=agg_func\n",
        "            )\n",
        "\n",
        "        # Generar Excel con el resultado\n",
        "        result_path = \"tabla_dinamica.xlsx\"\n",
        "        tabla_dinamica.to_excel(result_path)\n",
        "\n",
        "        # Generar mensaje de respuesta\n",
        "        summary = f\"### Tabla dinámica creada: '{pivot_query}'\\n\\n\"\n",
        "        summary += f\"Dimensiones: {tabla_dinamica.shape[0]} filas × {tabla_dinamica.shape[1]} columnas\\n\\n\"\n",
        "\n",
        "        if tabla_dinamica.shape[0] <= 10 and tabla_dinamica.shape[1] <= 5:\n",
        "            summary += \"Vista previa:\\n\\n```\\n\" + tabla_dinamica.to_string() + \"\\n```\"\n",
        "        else:\n",
        "            summary += \"La tabla es demasiado grande para mostrarla completa. Se ha generado un archivo Excel con los resultados.\"\n",
        "\n",
        "        new_messages = [\n",
        "            {\"role\": \"user\", \"content\": f\"Crear tabla dinámica '{pivot_query}'\"},\n",
        "            {\"role\": \"assistant\", \"content\": summary}\n",
        "        ]\n",
        "        history.extend(new_messages)\n",
        "        # Devolver todo el historial en lugar de solo los nuevos mensajes\n",
        "        return history, result_path\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error al crear tabla dinámica: {str(e)}\"\n",
        "        print(f\"[ERROR] {error_msg}\")\n",
        "        traceback.print_exc()\n",
        "        new_messages = [\n",
        "            {\"role\": \"user\", \"content\": f\"Crear tabla dinámica '{pivot_query}'\"},\n",
        "            {\"role\": \"assistant\", \"content\": error_msg}\n",
        "        ]\n",
        "        history.extend(new_messages)\n",
        "        # Devolver todo el historial en lugar de solo los nuevos mensajes\n",
        "        return history, None\n",
        "\n",
        "def set_query(selected):\n",
        "    return selected\n",
        "\n",
        "# Actualizar las opciones de columnas cuando se carga un archivo\n",
        "def update_column_options(file, api_key):\n",
        "    print(\"[INFO] Actualizando opciones de columnas...\")\n",
        "    if file is None:\n",
        "        print(\"[INFO] No hay archivo seleccionado\")\n",
        "        return gr.update(choices=[]), gr.update(choices=[]), \"Por favor, seleccione un archivo Excel\"\n",
        "\n",
        "    if not api_key:\n",
        "        print(\"[INFO] No se proporcionó API key\")\n",
        "        return gr.update(choices=[]), gr.update(choices=[]), \"Por favor, ingrese su API key\"\n",
        "\n",
        "    try:\n",
        "        # Configurar el agente y cargar el DataFrame\n",
        "        _, df, _ = setup_agent(api_key, file)\n",
        "\n",
        "        # Obtener la lista de columnas\n",
        "        column_list = df.columns.tolist()\n",
        "        print(f\"[INFO] Columnas detectadas: {column_list}\")\n",
        "\n",
        "        # Generar descripciones de columnas\n",
        "        column_descriptions = [f\"{col} ({df[col].dtype})\" for col in column_list]\n",
        "\n",
        "        # Actualizar las opciones de los combobox\n",
        "        message = f\"Se detectaron {len(column_list)} columnas en el archivo\"\n",
        "        return gr.update(choices=column_descriptions), gr.update(choices=column_descriptions), message\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error al cargar columnas: {str(e)}\"\n",
        "        print(f\"[ERROR] {error_msg}\")\n",
        "        return gr.update(choices=[]), gr.update(choices=[]), error_msg\n",
        "\n",
        "# Interfaz de Gradio\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Chatbot para análisis de datos con IA\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            api_key = gr.Textbox(\n",
        "                label=\"OpenAI API Key\",\n",
        "                type=\"password\",\n",
        "                value=\"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\"\n",
        "            )\n",
        "            file = gr.File(label=\"Subir Excel\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            send_full_history = gr.Checkbox(\n",
        "                label=\"Enviar toda la conversación al modelo\",\n",
        "                value=True,\n",
        "                info=\"Al activar esta opción, el modelo tendrá en cuenta todo el historial de la conversación para dar respuestas más contextualizadas. Esto mejora la coherencia pero puede aumentar el tiempo de respuesta.\"\n",
        "            )\n",
        "\n",
        "    # Chatbot principal - Usando el formato correcto de mensajes\n",
        "    chatbot = gr.Chatbot(label=\"Conversación\", elem_id=\"chatbot\", height=400, type=\"messages\")\n",
        "\n",
        "    with gr.Row():\n",
        "        query = gr.Textbox(label=\"Escribe tu consulta\", interactive=True, show_label=False,\n",
        "                          placeholder=\"Escribe tu mensaje...\")\n",
        "        btn_query = gr.Button(\"Enviar\")\n",
        "\n",
        "    # Herramientas principales\n",
        "    with gr.Row():\n",
        "        query_templates = gr.Dropdown(\n",
        "            choices=[\n",
        "                \"Mostrar las primeras 10 filas\",\n",
        "                \"Calcular el resultado promedio por categoría\",\n",
        "                \"Mostrar los 5 registros con mayor valor\",\n",
        "                \"Identificar valores atípicos\",\n",
        "                \"Calcular la suma total por columna\",\n",
        "                \"¿Cuál es la relación entre dos columnas?\",\n",
        "                \"Genera un resumen estadístico\"\n",
        "            ],\n",
        "            label=\"Consultas comunes\",\n",
        "            allow_custom_value=True  # Permitir valores personalizados\n",
        "        )\n",
        "\n",
        "    # Herramientas avanzadas en pestañas\n",
        "    tabs = gr.Tabs()\n",
        "    with tabs:\n",
        "        with gr.Tab(\"Perfil de Datos\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    btn_profile = gr.Button(\"Generar Perfil de Datos\")\n",
        "                    additional_info = gr.Textbox(label=\"Características adicionales\", placeholder=\"Ej: Incluir estadísticas de columnas específicas\")\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    **Perfil de datos rápido**: Genera un análisis completo del Excel, incluyendo estadísticas, valores nulos y tipos de datos, exportándolos en un archivo Excel bien estructurado.\n",
        "                    \"\"\")\n",
        "\n",
        "        comparison_tab = gr.Tab(\"Comparador de Columnas\")\n",
        "        with comparison_tab:\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    # Configurar los combobox para que sean dinámicos\n",
        "                    col1_dropdown = gr.Dropdown(label=\"Columna 1\", choices=[], value=None, interactive=True)\n",
        "                    col2_dropdown = gr.Dropdown(label=\"Columna 2\", choices=[], value=None, interactive=True)\n",
        "                    additional_conditions = gr.Textbox(label=\"Condiciones adicionales\", placeholder=\"Ej: Filtrar por valores mayores a 1000\")\n",
        "                    btn_compare = gr.Button(\"Comparar Columnas\")\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    **Comparador de columnas**: Permite seleccionar dos columnas cualquiera del Excel y muestra su relación estadística, incluyendo correlación (para numéricas) o tablas cruzadas (para categóricas), con visualizaciones.\n",
        "                    \"\"\")\n",
        "\n",
        "        with gr.Tab(\"Buscador Avanzado\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    search_query = gr.Textbox(label=\"¿Qué estás buscando?\", placeholder=\"Ej: Clientes con resultado mayor a 100000\")\n",
        "                    btn_search = gr.Button(\"Buscar\")\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    **Buscador avanzado**: Permite buscar en todo el Excel usando lenguaje natural. La IA traduce la consulta a código Python para encontrar exactamente lo que se necesita.\n",
        "                    \"\"\")\n",
        "\n",
        "        with gr.Tab(\"Filtrador Natural\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    filter_query = gr.Textbox(label=\"Describe tu filtro\", placeholder=\"Ej: Donde ingresos sean mayores que salidas\")\n",
        "                    btn_filter = gr.Button(\"Aplicar Filtro\")\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    **Filtrador natural**: Similar al buscador, pero enfocado en filtrar datos según criterios complejos expresados en lenguaje cotidiano.\n",
        "                    \"\"\")\n",
        "\n",
        "        # with gr.Tab(\"Tabla Dinámica\"):\n",
        "        #     with gr.Row():\n",
        "        #         with gr.Column():\n",
        "        #             pivot_query = gr.Textbox(label=\"Describe la tabla dinámica\", placeholder=\"Ej: Promedios de resultado por categoría de cliente\")\n",
        "        #             btn_pivot = gr.Button(\"Crear Tabla Dinámica\")\n",
        "        #         with gr.Column():\n",
        "        #             gr.Markdown(\"\"\"\n",
        "        #             **Generador de tablas dinámicas**: Crea tablas dinámicas complejas a partir de descripciones en lenguaje natural, simplificando un proceso que normalmente requiere conocimiento técnico.\n",
        "        #             \"\"\")\n",
        "\n",
        "    # Botones para guardar y descargar\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            btn_save = gr.Button(\"Guardar Conversación\")\n",
        "            download_link_txt = gr.File(label=\"Descargar Conversación\", interactive=False)\n",
        "\n",
        "        with gr.Column():\n",
        "            btn_download_excel = gr.Button(\"Descargar Excel de Resultados\")\n",
        "            download_link_excel = gr.File(label=\"Descargar Resultados\", interactive=False)\n",
        "\n",
        "    # Eventos para la funcionalidad principal\n",
        "    btn_query.click(query_agent,\n",
        "                   inputs=[api_key, file, query, send_full_history],\n",
        "                   outputs=[chatbot, download_link_excel, query])\n",
        "\n",
        "    query.submit(query_agent,\n",
        "                inputs=[api_key, file, query, send_full_history],\n",
        "                outputs=[chatbot, download_link_excel, query])\n",
        "\n",
        "    query_templates.change(set_query, inputs=[query_templates], outputs=[query])\n",
        "\n",
        "    btn_save.click(save_conversation, outputs=[download_link_txt])\n",
        "\n",
        "    btn_download_excel.click(generate_default_excel, outputs=[download_link_excel])\n",
        "\n",
        "    # Eventos para las nuevas herramientas\n",
        "    btn_profile.click(generate_data_profile,\n",
        "                     inputs=[file, api_key, additional_info],\n",
        "                     outputs=[chatbot, download_link_excel])\n",
        "\n",
        "    # Mensaje de estado para la actualización de columnas\n",
        "    column_status = gr.Textbox(label=\"Estado de las columnas\", interactive=False)\n",
        "\n",
        "    file.change(update_column_options,\n",
        "               inputs=[file, api_key],\n",
        "               outputs=[col1_dropdown, col2_dropdown, column_status])\n",
        "\n",
        "    # Función para extraer el nombre de la columna del formato extendido\n",
        "    def extract_column_name(column_desc):\n",
        "        return column_desc.split(' (')[0] if column_desc else None\n",
        "\n",
        "    def compare_columns_wrapper(file, api_key, col1_desc, col2_desc, additional_conditions):\n",
        "        col1 = extract_column_name(col1_desc)\n",
        "        col2 = extract_column_name(col2_desc)\n",
        "        return compare_columns(file, api_key, col1, col2, additional_conditions)\n",
        "\n",
        "    btn_compare.click(compare_columns_wrapper,\n",
        "                     inputs=[file, api_key, col1_dropdown, col2_dropdown, additional_conditions],\n",
        "                     outputs=[chatbot, download_link_excel])\n",
        "\n",
        "    btn_search.click(advanced_search,\n",
        "                    inputs=[file, api_key, search_query],\n",
        "                    outputs=[chatbot, download_link_excel])\n",
        "\n",
        "    btn_filter.click(natural_filter,\n",
        "                    inputs=[file, api_key, filter_query],\n",
        "                    outputs=[chatbot, download_link_excel])\n",
        "\n",
        "    # btn_pivot.click(generate_pivot_table,\n",
        "    #                inputs=[file, api_key, pivot_query],\n",
        "    #                outputs=[chatbot, download_link_excel])\n",
        "\n",
        "    # Evento para actualizar las columnas cuando se selecciona la pestaña del comparador\n",
        "    comparison_tab.select(\n",
        "        fn=update_column_options,\n",
        "        inputs=[file, api_key],\n",
        "        outputs=[col1_dropdown, col2_dropdown, column_status]\n",
        "    )\n",
        "\n",
        "# Lanzar la aplicación\n",
        "demo.launch(debug=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZcErdaE-YE3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWH8T/LGhl3fdMmX4M36Q5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}